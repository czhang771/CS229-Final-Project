experiment_name: "hyperparam_test"

training:
  epochs: 50
  num_games: 20
  game_length: 20
  k: 10
  gamma: 0.99
  learner_type: "policy_gradient"  # Options: policy_gradient, actor_critic

hyperparameters:
  learning_rates: [0.01, 0.001, 0.0005]
  scheduler_types: ["exponential", "step", "cosine"]
  optimizers: ["adamw", "sgd"]

opponents:
  training: 
    available_strategies: ["TFT", "Cu", "Du", "Random", "Cp", "ImpTFT", "GTFT", "GRIM", "WSLS"]
    num_opponents: 3 
  evaluation:
    - "Random"
    - "Cu"
    - "Du"

model:
  actor_model: "LSTM" # Options: MLP, LSTM, LogReg
  critic_model: "MLP"
  hidden_layers: [40, 40]  # Only applicable for MLP

strategy_params:
  Cp_p: 0.7        # Cp cooperates with probability 0.7
  ImpTFT_p: 0.85   # ImpTFT imitates opponent with probability 0.85
  GTFT_R: 3        # GTFT Reward when both cooperate
  GTFT_P: 1        # GTFT Punishment when both defect
  GTFT_T: 5        # GTFT Temptation (defect while other cooperates)
  GTFT_S: 0        # GTFT Suckerâ€™s payoff (cooperate while other defects)